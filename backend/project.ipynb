{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Baba\\pranish\\Ensemble_DL_Ransomware_Detector-master\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Baba\\pranish\\Ensemble_DL_Ransomware_Detector-master\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Baba\\pranish\\Ensemble_DL_Ransomware_Detector-master\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Baba\\pranish\\Ensemble_DL_Ransomware_Detector-master\\venv\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, math, string, pefile, time, threading\n",
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import sys\n",
    "import tkinter.filedialog\n",
    "from capstone import *\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras import layers, preprocessing\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter.filedialog import askopenfilenames\n",
    "from tkinter.ttk import Progressbar\n",
    "\n",
    "opModel = Sequential()\n",
    "\n",
    "opModel.add(layers.InputLayer(input_shape=(50,)))\n",
    "opModel.add(layers.Dense(256, activation='relu'))\n",
    "opModel.add(layers.BatchNormalization())\n",
    "opModel.add(layers.Dense(128, activation='relu'))\n",
    "opModel.add(layers.BatchNormalization())\n",
    "opModel.add(layers.Dense(64, activation='relu'))\n",
    "opModel.add(layers.BatchNormalization())\n",
    "opModel.add(layers.Dense(32, activation='relu'))\n",
    "opModel.add(layers.BatchNormalization())\n",
    "opModel.add(layers.Dense(16, activation='relu'))\n",
    "opModel.add(layers.BatchNormalization())\n",
    "opModel.add(layers.Dense(3, activation='softmax'))\n",
    "opModel.load_weights(\"weights-improvement-574-0.85.hdf5\")\n",
    "opModel.compile(optimizer=\"rmsprop\",\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "class histSequence(Sequence):\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = shuffle(x, y)\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        return np.array([\n",
    "        np.load(file_name)\n",
    "        for file_name in batch_x]), np.array(batch_y)\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "class histSequenceVal(histSequence):\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "model = Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(100, 100, 1)))\n",
    "model.add(layers.SpatialDropout2D(rate=0.2))\n",
    "model.add(layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SpatialDropout2D(rate=0.1))\n",
    "model.add(layers.Conv2D(16, kernel_size=3, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SpatialDropout2D(rate=0.1))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "class hashCorpusSequence(Sequence):\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = shuffle(x, y)\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        return np.array([\n",
    "        np.rint(((np.load(file_name) - np.min(np.load(file_name))) /\n",
    "        (np.max(np.load(file_name)) - np.min(np.load(file_name)))) * 255).astype(int)\n",
    "        for file_name in batch_x]), np.array(batch_y)\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "class hashCorpusSequenceVal(hashCorpusSequence):\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "model.load_weights(\"weights-improvement-04-0.72.hdf5\")\n",
    "model.compile(optimizer=\"adamax\",\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "opModel._name = \"opcodeModel\"\n",
    "model._name = \"stringsAsGreyscaleModel\"\n",
    "\n",
    "def ensemble(models, model_inputs):\n",
    "    outputs = [models[0](model_inputs[0]), models[1](model_inputs[1])]\n",
    "    y = layers.average(outputs)\n",
    "    modelEns = Model(model_inputs, y, name='ensemble')\n",
    "    return modelEns\n",
    "\n",
    "models = [opModel, model]\n",
    "model_inputs = [Input(shape=(50,)), Input(shape=(100, 100, 1))]\n",
    "modelEns = ensemble(models, model_inputs)\n",
    "modelEns.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "def strings(filename, min=4):\n",
    "    with open(filename, errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "        result = \"\"\n",
    "        for c in f.read():\n",
    "            if c in string.printable:\n",
    "                result += c\n",
    "                continue\n",
    "            if len(result) >= min:\n",
    "                yield result\n",
    "            result = \"\"\n",
    "        if len(result) >= min: # catch result at EOF\n",
    "            yield result\n",
    "            \n",
    "def wordSequence(pePath):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        for s in strings(pePath):\n",
    "            text += s + \"\\n\"\n",
    "        sequence = preprocessing.text.text_to_word_sequence(text)[:10000]\n",
    "        return sequence\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def hashWordSequences(sequences, maxSeqLen, vocabSize):\n",
    "    hashedSeqs = []\n",
    "    docCount = 0\n",
    "    for sequence in sequences:\n",
    "        try:\n",
    "            text = \" \".join(sequence)\n",
    "            hashWordIDs = preprocessing.text.hashing_trick(text, round(vocabSize * 1.5), hash_function='md5')\n",
    "            docLen = len(hashWordIDs)\n",
    "            if docLen < maxSeqLen:\n",
    "                hashWordIDs += [0 for i in range(0, maxSeqLen-docLen)]\n",
    "            hashWordIDs = np.array(hashWordIDs).reshape(100, 100, 1)\n",
    "            hashedSeqs.append(hashWordIDs)\n",
    "            docCount += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return hashedSeqs\n",
    "\n",
    "def preprocessPEs(pePaths):\n",
    "    mlInputs = []\n",
    "    opCodeSet = set()\n",
    "    opCodeDicts = []\n",
    "    opCodeFreqs = {}\n",
    "    count = 1\n",
    "    for sample in pePaths:\n",
    "        try:\n",
    "            pe = pefile.PE(sample, fast_load=True)\n",
    "            entryPoint = pe.OPTIONAL_HEADER.AddressOfEntryPoint\n",
    "            data = pe.get_memory_mapped_image()[entryPoint:]\n",
    "            cs = Cs(CS_ARCH_X86, CS_MODE_32)\n",
    "            opcodes = []\n",
    "            \n",
    "            for i in cs.disasm(data, 0x1000):\n",
    "                opcodes.append(i.mnemonic)\n",
    "                \n",
    "            opcodeDict = {}\n",
    "            total = len(opcodes)\n",
    "            \n",
    "            opCodeSet = set(list(opCodeSet) + opcodes)\n",
    "            for opcode in opCodeSet:\n",
    "                freq = 1\n",
    "                for op in opcodes:\n",
    "                    if opcode == op:\n",
    "                        freq += 1\n",
    "                try: \n",
    "                    opCodeFreqs[opcode] += freq\n",
    "                except:\n",
    "                    opCodeFreqs[opcode] = freq\n",
    "                opcodeDict[opcode] = round((freq / total) * 100, 2)\n",
    "                \n",
    "            opCodeDicts.append(opcodeDict)\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    opCodeFreqsSorted = np.genfromtxt(\"top50opcodes.csv\", delimiter=\",\", dtype=\"str\")[1:, 0]\n",
    "    count = 0\n",
    "    \n",
    "    for opDict in opCodeDicts:\n",
    "        opFreqVec = []\n",
    "        for opcode in opCodeFreqsSorted[:50]:\n",
    "            try: opFreqVec.append(opDict[opcode])\n",
    "            except Exception as e:\n",
    "                if str(type(e)) == \"<class 'KeyError'>\":\n",
    "                    opFreqVec.append(0.0)\n",
    "        mlInputs.append([np.array(opFreqVec)])\n",
    "        count += 1\n",
    "        \n",
    "    sequences = []\n",
    "    count = 0\n",
    "    for sample in pePaths:\n",
    "        sequences.append(wordSequence(sample))\n",
    "        count += 1\n",
    "\n",
    "    with open(\"finalVocabSize.txt\", \"r\") as f:\n",
    "        maxVocabSize = int(f.readline())\n",
    "    \n",
    "    hashSeqs = hashWordSequences(sequences, 10000, maxVocabSize)\n",
    "    \n",
    "    count = 0\n",
    "    for hashSeq in hashSeqs:\n",
    "        mlInputs[count].append(np.array(hashSeq))\n",
    "        count += 1\n",
    "        \n",
    "    mlInputs = np.array(mlInputs)\n",
    "    return mlInputs\n",
    "\n",
    "def predictPEs(pePaths):\n",
    "    classNames = [\"benign\", \"malware\", \"ransomware\"]\n",
    "    pePredictions = {}\n",
    "    count = 0\n",
    "    \n",
    "    for pePath in pePaths:\n",
    "        x1 = preprocessPEs(pePaths)[count][0].reshape(1, 50)\n",
    "        x2 = preprocessPEs(pePaths)[count][1].reshape(1, 100, 100, 1)\n",
    "        count += 1\n",
    "        pePredictions[pePath] = classNames[np.argmax(modelEns.predict(x=[x1, x2]))]\n",
    "    return pePredictions\n",
    "\n",
    "def select_files():\n",
    "    try: \n",
    "        pePaths = list(filedialog.askopenfilenames(filetypes=[(\"Windows executable files\", \"*.exe\")]))\n",
    "        if len(pePaths) == 0:\n",
    "            raise ValueError(\"No files selected.\")\n",
    "        preds = predictPEs(pePaths)\n",
    "        \n",
    "        classificationsStr = \"\"\n",
    "        for key in preds.keys():\n",
    "            classificationsStr += \"'\" + key + \"'\" + \" detected as \" + preds[key] + \"\\n\\n\"\n",
    "        resultsWindow = tk.Toplevel(root)\n",
    "        resultsWindow.title(\"Detections\")\n",
    "        resultsText = tk.Text(resultsWindow, height=20, width=50)\n",
    "        resultsText.pack(padx=10, pady=10)\n",
    "        resultsText.insert(tk.END, classificationsStr)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", \"Error: \" + str(e) + \"\\nPlease try again...\")\n",
    "        \n",
    "root = tk.Tk()\n",
    "root.title(\"Processing files...\")\n",
    "button = tk.Button(root, text=\"Select files\", command=select_files)\n",
    "button.pack(padx=50, pady=50)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
